<style>
  .center {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 80%; /* or any desired width */
  }
  table {
  width: 100%;
  border-collapse: collapse;
  margin-bottom: 1rem;
  }

  caption {
    font-weight: bold;
    margin-bottom: 0.5rem;
  }

  th, td {
    border: 1px solid #ddd;
    padding: 0.5rem;
    text-align: center;
  }

  thead th {
    background-color: #f2f2f2;
  }

  table th, table td {
    text-align: center;
  }
  
</style>

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Optimizing Agentic RAG with Process Supervision">
  <meta name="keywords" content="RAG, medicine, question answering">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/guangzhi-xiong-47a299251">Guangzhi Xiong</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://andy-jqa.github.io/">Qiao Jin</a><sup>2*</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/xiao-wang-6143a928b">Xiao Wang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://fangyinfff.github.io/">Yin Fang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://liuhl2000.github.io/">Haolin Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://yifanyang.dev/">Yifan Yang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/fangyuan-chen-45271816a/">Fangyuan Chen</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/zhixing-song-284772186">Zhixing Song</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.linkedin.com/in/dengyuwang">Dengyu Wang</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="https://minjiazhang.github.io/">Minjia Zhang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.ncbi.nlm.nih.gov/research/bionlp/Zhiyong-Lu">Zhiyong Lu</a><sup>2‚Ä†</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.virginia.edu/~az9eg/website/home.html">Aidong Zhang</a><sup>1‚Ä†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Department of Computer Science, University of Virginia</span><br>
            <span class="author-block"><sup>2</sup>National Library of Medicine, National Institutes of Health</span><br>
            <span class="author-block"><sup>3</sup>Department of Computer Science, University of Illinois Urbana-Champaign</span><br>
            <span class="author-block"><sup>4</sup>Medical Oncology, Dana-Farber Cancer Institute</span><br>
            <span class="author-block"><sup>5</sup>Surgery, University of Alabama at Birmingham</span><br>
            <span class="author-block"><sup>6</sup>Department of Neurology, Yale School of Medicine</span><br>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">*Equal Contribution,</span>
            <span class="author-block">‚Ä†Co-correspondence</span><br>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./RAG_Gym_Arxiv.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/RAG-Gym/RAG-Gym"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Models Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/RAG-Gym"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:18px">ü§ó</p>
                  </span>
                  <span>Models</span>
                  </a>
              </span>
              <!-- Twitter Link. -->
              <span class="link-block">
                <a href="https://twitter.com/GuangzhiXiong/status/1892403975015526450"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">üåê</p>
                  </span>
                  <span>Twitter</span>
                </a>
              </span> 
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Retrieval-augmented generation (RAG) has shown great promise for knowledge-intensive tasks and recently advanced with agentic RAG, where language agents engage in multi-round interactions with external knowledge sources for adaptive information retrieval. However, existing agentic RAG methods often depend on ad-hoc prompt engineering and lack a unified optimization framework.
          </p>
          
          <p>
            We introduce <b>RAG-Gym</b>, a comprehensive platform that systematically explores three optimization dimensions: (1) prompt engineering, (2) actor tuning, and (3) critic training. For prompt engineering, we propose Re¬≤Search, a novel agent incorporating reasoning reflection that significantly outperforms standard prompts. In actor tuning, we evaluate three popular post-training algorithms with fine-grained process supervision and identify direct preference optimization as the most effective. We further demonstrate that a trained critic can enhance inference by selecting higher-quality intermediate reasoning steps.
          </p>
          
          <p>
            Together, these findings lead to the optimized <b>Re¬≤Search++</b> agent, which surpasses most recent methods like Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we examine the impact of different reward sources and analyze scaling properties in training and inference, offering practical insights for agentic RAG optimization.
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">

      <div class="column is-four-fifths">
        <h3 class="title is-3">RAG-Gym Framework</h3>
        <div class="content has-text-justified">
          <p>
            Here is an overview of the RAG-Gym framework. RAG-Gym employs a modular design, comprising <em>prompt engineering</em>, <em>actor tuning</em>, and <em>critic training</em>, to systematically optimize agentic RAG performance. By leveraging all three components, RAG-Gym improves the F1 score of the ReAct agent on HotpotQA from 41.09% to 60.19%.
          </p>
        </div>
      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <img src="./figs/rag_gym.png" alt="RAG-Gym Overview" class="center" style="max-width: 100%; height: auto;"/>
      </div>
    </div>

  </div>
  
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">

      <div class="column is-four-fifths">
        <h3 class="title is-4">üèãÔ∏è Dimension 1: Prompt Engineering</h3>
        <div class="content has-text-justified">
          <p>
            Effective prompts are key to guiding LLM behavior. Building on a summary of existing agent functions‚Äîsuch as answer generation, question reasoning, and query generation‚ÄîRAG-Gym introduces a novel agent architecture called Re¬≤Search (Reasoning, Reflection, and Search).

            The core innovation of Re¬≤Search lies in its unique "reasoning reflection" mechanism. Before making a final decision, the agent will:
            <ol>
              <li>Construct an initial reasoning process and answer based on all currently available information.</li>
              <li>Reflect on its reasoning chain to identify which statements lack support from current information or are unverified claims.</li>
              <li>Generate highly targeted search queries based on these "uncertainties" to acquire missing key information and improve the answer.</li>
            </ol>
            This design tightly integrates the search process with answer construction. Here is the comparison of the functional components in various agent architectures. Re¬≤Search is the only agent that incorporates all six key components, including "reasoning reflection".<br>
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <img src="./figs/compare.png" alt="Agent Comparison" class="center" style="max-width: 100%; height: auto;"/>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Extensive tests across models and datasets show that prompt engineering alone gives Re¬≤Search a clear advantage over standard prompts. For example, on HotpotQA, zero-shot ReAct achieves 41.09% F1, while Re¬≤Search reaches 44.91%.
          </p>
        </div>
      </div>
    </div>

  </div>

</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">

      <div class="column is-four-fifths">
        <h3 class="title is-4">üí™ Dimension 2: Actor Tuning</h3>
        <div class="content has-text-justified">
          <p>
            The "actor" refers to the LLM itself, and optimizing its parameters is crucial for improving decision quality. RAG-Gym enables fine-grained process supervision for actor tuning, meaning we not only consider the correctness of the final answer but also evaluate and reward each intermediate decision‚Äîsuch as generated search queries or reasoning steps.

            We systematically evaluate three mainstream LLM post-training algorithms: supervised fine-tuning (SFT), direct preference optimization (DPO), and proximal policy optimization (PPO). Our experiments show that for agents like ReAct, Search-o1, and Re¬≤Search‚Äîwhich require multi-step reasoning and interaction with the environment‚ÄîDPO and PPO generally outperform SFT, delivering more significant performance gains across most tasks. DPO, in particular, leverages preference comparisons between positive and negative actions to more effectively guide the model in generating high-quality intermediate steps. For example, after DPO tuning, Re¬≤Search improves its F1 score on HotpotQA from 44.91% (zero-shot) to 55.22%.
          </p>
        </div>
      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <img src="./figs/actor_tuning.png" alt="Actor Tuning" class="center" style="max-width: 100%; height: auto;"/>
      </div>
    </div>

  </div>
  
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">

      <div class="column is-four-fifths">
        <h3 class="title is-4">üßê Dimension 3: Critic Training</h3>
        <div class="content has-text-justified">
          <p>
            Beyond optimizing the actor, RAG-Gym also introduces a "critic" model‚Äîan external evaluator trained to predict the process reward for each state-action pair. The critic assesses the quality of actions (such as generated search queries) produced by the agent at each step.

            During inference, the actor generates multiple candidate actions for the current state. The critic scores these candidates, and the system selects the action with the highest score for execution. This mechanism offers several key advantages:
            <ul>
              <li><b>Improved generalizability:</b> Our experiments show that integrating a trained critic consistently boosts performance across various models‚Äîincluding base LLMs (like Llama-3.1-8B), DPO-tuned LLMs, and even models such as GPT-4o-mini‚Äîon multiple datasets.</li>
              <li><b>Plug-and-play enhancement:</b> The critic can be used as a standalone module to enhance LLMs that cannot be directly fine-tuned (e.g., closed-source models), providing an effective way to improve their RAG capabilities.</li>
            </ul>
          </p>
        </div>
      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <img src="./figs/critic_training.png" alt="Critic Training" class="center" style="max-width: 100%; height: auto;"/>
      </div>
    </div>

  </div>
  
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">

      <div class="column is-four-fifths">
        <h3 class="title is-4">üèÜ Re¬≤Search++: Optimized Agent Across All Dimensions</h3>
        <div class="content has-text-justified">
          <p>
            By integrating the best practices from all three optimization dimensions‚Äîadopting the Re¬≤Search agent architecture, tuning the actor with DPO, and leveraging a Critic for action selection during inference‚Äîwe developed the optimized Re¬≤Search++ agent.

            Compared to recent reinforcement learning methods that rely on outcome supervision (such as Search-R1 and R1-Searcher, which typically require thousands of training questions), Re¬≤Search++ demonstrates clear advantages. It not only matches or surpasses these methods on their reported training domains (e.g., HotpotQA), but also achieves substantial improvements on out-of-domain datasets (e.g., Bamboogle), with a relative F1 increase of 3.2% to 11.6% on average. This highlights the strong generalization capability enabled by RAG-Gym's fine-grained process supervision, effectively mitigating the overfitting issues that can arise from relying solely on outcome-based rewards.
          </p>
        </div>
      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <img src="./figs/main_results.png" alt="Main Results" class="center" style="max-width: 100%; height: auto;"/>
      </div>
    </div>

  </div>
  
</section>

<section class="section">
  <div class="container is-max-desktop">
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-3">Analysis and Discussion</h3>
        <div class="content has-text-justified">
          <h4 class="title is-4">Comparison of Different Reward Sources</h4>
          <p>
            Process rewards can be collected from different sources. We evaluated their effectiveness in guiding agent actions toward correct answers and their alignment with human preferences, comparing GPT-4o annotations, Llama-3.1-8B annotations, and rollout-based annotations using Math-Shepherd, alongside human expert annotations on MedQA. The results in the table below show that the GPT-4o-trained reward model delivers the highest performance across all datasets, providing precise, fine-grained rewards for agent optimization and achieving the strongest agreement with human experts (85.85%). Although Llama-3.1-8B and rollout-based annotations outperform random baselines, they remain less effective than GPT-4o annotations and can even underperform on general-domain questions. These findings highlight the limitations of rollout-based methods‚Äîoriginally designed for math reasoning‚Äîin complex reasoning and search tasks, and underscore the need for tailored approaches in agentic RAG.
          </p>
        </div>
      </div>
    </div>
    
    <div class="columns is-centered">
      <div class="column is-four-fifths">
      <table class="table">
        <thead>
        <tr>
          <th class="has-text-centered">Type</th>
          <th class="has-text-centered">Outcome Source</th>
          <th class="has-text-centered">Process Source</th>
          <th class="has-text-centered">HotpotQA<br>(EM / F1)</th>
          <th class="has-text-centered">2WikiMultihopQA<br>(EM / F1)</th>
          <th class="has-text-centered">Bamboogle<br>(EM / F1)</th>
          <th class="has-text-centered">MedQA<br>(Acc / Agree)</th>
        </tr>
        </thead>
        <tbody>
        <tr>
          <td class="has-text-centered">ORM</td>
          <td class="has-text-centered">Truth</td>
          <td class="has-text-centered">--</td>
          <td class="has-text-centered">41.10 / 53.35</td>
          <td class="has-text-centered">47.70 / 55.59</td>
          <td class="has-text-centered">43.20 / 57.46</td>
          <td class="has-text-centered">66.77 / --</td>
        </tr>
        <tr>
          <td class="has-text-centered">PRM (Random)</td>
          <td class="has-text-centered">--</td>
          <td class="has-text-centered">--</td>
          <td class="has-text-centered">32.20 / 42.83</td>
          <td class="has-text-centered">35.70 / 42.00</td>
          <td class="has-text-centered">38.40 / 47.86</td>
          <td class="has-text-centered">68.26 / 50.00</td>
        </tr>
        <tr>
          <td class="has-text-centered">PRM (Rollout)</td>
          <td class="has-text-centered">Truth</td>
          <td class="has-text-centered">Rollout</td>
          <td class="has-text-centered">39.60 / 51.85</td>
          <td class="has-text-centered">42.94 / 49.57</td>
          <td class="has-text-centered">48.80 / 56.05</td>
          <td class="has-text-centered">68.34 / 71.03</td>
        </tr>
        <tr>
          <td class="has-text-centered">PRM (Llama)</td>
          <td class="has-text-centered">Truth</td>
          <td class="has-text-centered">Llama-3.1-8B</td>
          <td class="has-text-centered">40.30 / 51.74</td>
          <td class="has-text-centered">40.70 / 48.22</td>
          <td class="has-text-centered">44.80 / 54.36</td>
          <td class="has-text-centered">68.50 / 65.99</td>
        </tr>
        <tr>
          <td class="has-text-centered">PRM (GPT)</td>
          <td class="has-text-centered">Truth</td>
          <td class="has-text-centered">GPT-4o</td>
          <td class="has-text-centered"><b>44.10 / 56.84</b></td>
          <td class="has-text-centered"><b>50.20 / 57.94</b></td>
          <td class="has-text-centered"><b>51.20 / 63.15</b></td>
          <td class="has-text-centered"><b>71.96 / 85.85</b></td>
        </tr>
        </tbody>
      </table>
      </div>
    </div>

    
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="columns">

          <div class="column">
            <div class="content">
              <h4 class="title is-4">Training Time Scaling</h4>
              <p>
                This figure displays how the performance of Re¬≤Search agents scales with the availability of more training data across four datasets. In general, the performance of Re¬≤Search improves with an increasing number of training samples, but the gains tend to converge as the sample size grows.  Notably, on HotpotQA, 2WikiMultihopQA, and Bambooglem, even a small amount of process reward data (250 samples) yield significant performance gains.
              </p>
            </div>
            <div class="columns is-centered">
              <div class="column is-full has-text-centered">
                <img src="./figs/train_scale.png" alt="Training Time Scaling" class="center" style="width: 100%; max-width: 800px; height: auto;"/>
              </div>
            </div>
          </div>
          
          <div class="column">
            <div class="content">
              <h4 class="title is-4">Inference Time Scaling</h4>
              <p>
                The results below show how the agent performance changes with the increasing number of sampled actions at each time step. We observe a consistent trend across multiple benchmarks, where increasing the number of sampled actions generally improves performance. However, performance gains gradually diminish, indicating that the agent reaches a point where additional sampled actions contribute less to improvement. 
                <!-- This suggests that while action sampling is beneficial, there is a limit to how much additional sampling enhances decision-making. -->
              </p>
            </div>
            <div class="columns is-centered">
              <div class="column is-full has-text-centered">
                <img src="./figs/test_scale.png" alt="Inference Time Scaling" class="center" style="width: 100%; max-width: 800px; height: auto;"/>
              </div>
            </div>
          </div>

        </div>
      </div>
    </div>


  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{xiong2025rag,
  title={Rag-gym: Optimizing reasoning and search agents with process supervision},
  author={Xiong, Guangzhi and Jin, Qiao and Wang, Xiao and Fang, Yin and Liu, Haolin and Yang, Yifan and Chen, Fangyuan and Song, Zhixing and Wang, Dengyu and Zhang, Minjia and others},
  journal={arXiv preprint arXiv:2502.13957},
  year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./RAG_Gym_Arxiv.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/RAG-Gym/RAG-Gym" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://mathvista.github.io/">MathVista</a>, licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
